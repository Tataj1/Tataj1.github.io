
Today I deepened my understanding of building end-to-end machine learning pipelines using Python and XGBoost. I reviewed several educational videos that covered essential concepts like Exploratory Data Analysis (EDA), data cleaning, model training, and evaluation. I learned how EDA helps detect missing values, outliers, and data imbalances before model training.

I also studied the importance of splitting data into training and testing sets to avoid overfitting and ensure that the model generalizes well. I reviewed how XGBoost works using sequential decision trees and why it's popular due to speed, accuracy, and built-in regularization.

Additionally, I learned about hyperparameter tuning techniques such as grid search and random search to optimize model performance. I created a Kahoot-style quiz for self-assessment and reviewed key XGBoost hyperparameters like learning_rate, max_depth, and n_estimators.

On the coding side, I troubleshooted a NameError in a Jupyter notebook where a DataFrame variable was incorrectly referenced as df_encode instead of df or df_encoded. This reinforced the importance of tracking variable names when working with transformed datasets.
