---
layout: post
title: "Day 11 â€“ End-to-End ML Pipelines with XGBoost and EDA"
date: 2025-06-10
author: Tahia Tajnim
permalink: /day11.html
tags: ["XGBoost", "Exploratory Data Analysis", "Pandas", "Flight Delay Dataset", "Machine Learning", "Python Practice", "CEAMLS Summer AI", "Exam Prep", "Train/Test Split", "Hyperparameter Tuning"]  

what_i_learned: |
  Today I deepened my understanding of building end-to-end machine learning pipelines using Python and XGBoost. I reviewed several educational videos that covered essential concepts like Exploratory Data Analysis (EDA), data cleaning, model training, and evaluation. I learned how EDA helps detect missing values, outliers, and data imbalances before model training. I also studied the importance of splitting data into training and testing sets to avoid overfitting and ensure that the model generalizes well. I reviewed how XGBoost works using sequential decision trees and why it's popular due to speed, accuracy, and built-in regularization. Additionally, I learned about hyperparameter tuning techniques such as grid search and random search to optimize model performance. I played a Kahoot quiz for self-assessment and reviewed key XGBoost hyperparameters like learning_rate, max_depth, and n_estimators. On the coding side, I troubleshooted a NameError in a Jupyter notebook where a DataFrame variable was incorrectly referenced as df_encode instead of df or df_encoded. This reinforced the importance of tracking variable names when working with transformed datasets.
  
blockers: |  
  Initially, I faced a NameError due to using an undefined variable name df_encode. After inspecting the code, I realized I should have used df or explicitly defined df_encoded = df.copy() before using it. This helped clarify best practices when creating modified versions of DataFrames during preprocessing.
  
reflection: |
  Today was a productive and focused session on core machine learning workflow topics. Breaking down the tutorial videos and summarizing them not only helped me prepare for my upcoming task but also clarified how EDA and XGBoost fit into real-world data science pipelines. Writing the Kahoot quiz helped reinforce my knowledge in a fun and interactive way. The debugging task reminded me of the importance of clean, consistent variable naming, especially when working in google colab. Overall, I feel more confident in both my practical coding skills and theoretical understanding of machine learning.
---


