---
layout: post
title: "Day 34 – Making AI Explain Itself"

date: 2025-07-11
author: Tahia Tajnim
permalink: /day34.html
tags: ["XAI", "SHAP", "LIME", "Flight Delay Prediction", "Presentation"]   

what_i_learned: |
  Today I explored Explainable AI (XAI) techniques, specifically SHAP (SHapley Additive Explanations) and LIME (Local Interpretable Model-agnostic Explanations). I learned how these tools help interpret machine learning model predictions by showing which features contribute most to each decision. SHAP is based on game theory and gives both global and local insights, while LIME provides easy-to-understand local explanations. These tools are essential for understanding why the model predicts certain flight delays, making the model more transparent and reliable. I also worked on creating our weekly presentation slide and recorded the summary video to reflect all our team’s progress for the week. This helped us review everything we achieved and explain our work clearly to others.
  
blockers: |  
  Installing and running SHAP on large datasets was a bit slow, and I had to troubleshoot visualization compatibility with my Python setup. I’m still figuring out how to cleanly display SHAP values inside the dashboard.
  
reflection: |
  Using SHAP and LIME gave me a deeper understanding of how my model thinks. It’s not just about getting high accuracy  it's about knowing why the model makes its decisions. This step is critical in building trust and usability for real-world applications, especially in air traffic management. I feel more confident now integrating these explanations into my dashboard to support better decision-making. Working on the weekly slide and video also made me realize how far we've come — it was a great way to reflect, practice communication, and organize our thoughts clearly for others to understand.
---

